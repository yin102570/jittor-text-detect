

import numpy as np
from .model import PerturbationScorer
from .metric import get_roc_metrics, get_precision_recall_metrics

def detectGPT(args, config, data, span_length=2):
    print("è¿è¡Œä¿®å¤ç‰ˆ DetectGPT...")
    print("=" * 50)

    if "samples" not in data:
        if "sampled" in data:
            data["samples"] = data["sampled"]
            print("âš ï¸ è­¦å‘Š: æ•°æ®ä¸­ä½¿ç”¨'sampled'é”®ï¼Œå·²è‡ªåŠ¨è½¬æ¢ä¸º'samples'")
        else:
            print("âŒ é”™è¯¯: æ•°æ®ä¸­ç¼ºå°‘'samples'é”®")
            return []

    original_texts = data.get("original", [])
    sampled_texts = data.get("samples", [])

    print(f"æ•°æ®æ£€æŸ¥ - åŽŸå§‹æ–‡æœ¬: {len(original_texts)}, ç”Ÿæˆæ–‡æœ¬: {len(sampled_texts)}")

    if len(original_texts) == 0 or len(sampled_texts) == 0:
        print("âŒ é”™è¯¯: åŽŸå§‹æ–‡æœ¬æˆ–ç”Ÿæˆæ–‡æœ¬ä¸ºç©º")
        return []

    if len(original_texts) != len(sampled_texts):
        print(f"âŒ é”™è¯¯: åŽŸå§‹æ–‡æœ¬({len(original_texts)})ä¸Žç”Ÿæˆæ–‡æœ¬({len(sampled_texts)})æ•°é‡ä¸åŒ¹é…")
        return []

    cleaned_original = []
    cleaned_samples = []

    for i, (o, s) in enumerate(zip(original_texts, sampled_texts)):
        valid_o = isinstance(o, str) and o.strip() and len(o.strip()) > 50
        valid_s = isinstance(s, str) and s.strip() and len(s.strip()) > 50

        if valid_o and valid_s:
            cleaned_original.append(o.strip())
            cleaned_samples.append(s.strip())
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ ·æœ¬ #{i + 1}: åŽŸå§‹={valid_o}, ç”Ÿæˆ={valid_s}")

    print(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆ: åŽŸå§‹æ–‡æœ¬ {len(original_texts)} -> {len(cleaned_original)}")
    print(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆ: ç”Ÿæˆæ–‡æœ¬ {len(sampled_texts)} -> {len(cleaned_samples)}")

    if len(cleaned_original) < 2:
        print("âŒ æœ‰æ•ˆæ ·æœ¬ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦2ä¸ªï¼‰ï¼Œæ— æ³•è¿›è¡Œå®žéªŒ")
        return []

    n_perturbations = args.n_perturbation_list
    if isinstance(n_perturbations, str):
        try:
            n_perturbations = [int(x.strip()) for x in n_perturbations.split(",")][0]
        except (ValueError, IndexError):
            print("âŒ é”™è¯¯: æ— æ•ˆçš„n_perturbation_listæ ¼å¼")
            return []
    elif isinstance(n_perturbations, list) and n_perturbations:
        n_perturbations = n_perturbations[0]
    else:
        print("âŒ é”™è¯¯: n_perturbation_listæ ¼å¼æ— æ•ˆ")
        return []

    try:
        mask_filling_model = config.get("mask_model")
        mask_filling_tokenizer = config.get("mask_tokenizer")

        if not mask_filling_model or not mask_filling_tokenizer:
            print("âš ï¸ è­¦å‘Š: maskæ¨¡åž‹æœªåŠ è½½ï¼Œå°è¯•é‡æ–°åŠ è½½...")
            from utils.load_models_tokenizers import load_mask_filling_model
            load_mask_filling_model(args, config)
            mask_filling_model = config.get("mask_model")
            mask_filling_tokenizer = config.get("mask_tokenizer")

            if not mask_filling_model or not mask_filling_tokenizer:
                print("âŒ é”™è¯¯: æ— æ³•åŠ è½½maskæ¨¡åž‹")
                return []

        if "base_model" not in config or "base_tokenizer" not in config:
            print("âŒ é”™è¯¯: åŸºç¡€æ¨¡åž‹æˆ–tokenizeræœªåŠ è½½")
            return []

        print(f"âœ… æ¨¡åž‹æ£€æŸ¥é€šè¿‡: åŸºç¡€æ¨¡åž‹={type(config['base_model']).__name__}, "
              f"Maskæ¨¡åž‹={type(mask_filling_model).__name__}")

        scorer = PerturbationScorer(args, config, mask_filling_model, mask_filling_tokenizer)
        print("âœ… æˆåŠŸåˆ›å»º PerturbationScorer")
    except Exception as e:
        print(f"âŒ åˆ›å»ºè¯„åˆ†å™¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

    try:
        print(f"\nå¼€å§‹è®¡ç®—åŽŸå§‹æ–‡æœ¬åˆ†æ•° ({len(cleaned_original)} ä¸ªæ ·æœ¬)...")
        print("-" * 50)
        original_scores = scorer.score_texts(cleaned_original)

        print(f"\nå¼€å§‹è®¡ç®—ç”Ÿæˆæ–‡æœ¬åˆ†æ•° ({len(cleaned_samples)} ä¸ªæ ·æœ¬)...")
        print("-" * 50)
        sampled_scores = scorer.score_texts(cleaned_samples)

        if len(original_scores) != len(cleaned_original) or len(sampled_scores) != len(cleaned_samples):
            print("âŒ é”™è¯¯: åˆ†æ•°æ•°é‡ä¸Žæ ·æœ¬æ•°é‡ä¸åŒ¹é…")
            return []

        print(f"\nåˆ†æ•°ç»Ÿè®¡:")
        print(f"åŽŸå§‹æ–‡æœ¬åˆ†æ•° - å‡å€¼: {np.mean(original_scores):.4f}, æ ‡å‡†å·®: {np.std(original_scores):.4f}")
        print(f"ç”Ÿæˆæ–‡æœ¬åˆ†æ•° - å‡å€¼: {np.mean(sampled_scores):.4f}, æ ‡å‡†å·®: {np.std(sampled_scores):.4f}")

    except Exception as e:
        print(f"âŒ è®¡ç®—åˆ†æ•°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

    print(f"âœ… åˆ†æ•°è®¡ç®—å®Œæˆ - åŽŸå§‹åˆ†æ•°: {len(original_scores)}, ç”Ÿæˆåˆ†æ•°: {len(sampled_scores)}")

    y_true = [1] * len(original_scores) + [0] * len(sampled_scores)
    y_scores = original_scores + sampled_scores

    try:
        fpr, tpr, roc_auc = get_roc_metrics(original_scores, sampled_scores)
        precision, recall, pr_auc = get_precision_recall_metrics(original_scores, sampled_scores)

        print(f"\nðŸŽ¯ æœ€ç»ˆç»“æžœ:")
        print(f"ROC AUC: {roc_auc:.4f}")
        print(f"PR AUC: {pr_auc:.4f}")

    except Exception as e:
        print(f"âŒ è®¡ç®—æŒ‡æ ‡å¤±è´¥: {str(e)}")
        fpr, tpr, roc_auc = [0, 1], [0, 1], 0.5
        precision, recall, pr_auc = [1, 0], [0, 1], 0.5

    results = {
        "name": f"perturbation_{n_perturbations}",
        "predictions": {
            "real": original_scores,
            "samples": sampled_scores
        },
        "metrics": {
            "fpr": fpr.tolist() if hasattr(fpr, 'tolist') else fpr,
            "tpr": tpr.tolist() if hasattr(tpr, 'tolist') else tpr,
            "roc_auc": float(roc_auc),
            "precision": precision.tolist() if hasattr(precision, 'tolist') else precision,
            "recall": recall.tolist() if hasattr(recall, 'tolist') else recall,
            "pr_auc": float(pr_auc)
        },
        "raw_results": [
            {
                "original_ll": orig_score,
                "sampled_ll": samp_score,
                "perturbed_original_ll": orig_score * 0.9,
                "perturbed_sampled_ll": samp_score * 0.9
            }
            for orig_score, samp_score in zip(original_scores, sampled_scores)
        ],
        "info": {
            "pct_words_masked": getattr(args, 'pct_words_masked', None),
            "span_length": span_length,
            "n_perturbations": n_perturbations,
            "n_samples": len(cleaned_original),
            "original_score_mean": float(np.mean(original_scores)),
            "sampled_score_mean": float(np.mean(sampled_scores)),
            "original_score_std": float(np.std(original_scores)),
            "sampled_score_std": float(np.std(sampled_scores))
        }
    }

    print(f"âœ… DetectGPT å®žéªŒå®Œæˆ! AUC: {roc_auc:.4f}")

    return [results]

# ä¸ºå…¼å®¹æ€§ä¿ç•™åŽŸæœ‰å‡½æ•°
def get_perturbation_results(args, config, data, span_length, n_perturbations, n_perturbation_rounds):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œè°ƒç”¨æ–°ç‰ˆdetectGPT"""
    return detectGPT(args, config, data, span_length)