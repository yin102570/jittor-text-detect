import os
import json
import datetime


def initial_setup(args, config):
    API_TOKEN_COUNTER = 0  # ä¿ç•™è¯¥å˜é‡ï¼Œä¿æŒconfigå…¼å®¹æ€§ï¼Œæ— å®é™…OpenAIç”¨é€”

    # å½»åº•åˆ é™¤æ‰€æœ‰OpenAIç›¸å…³åˆ¤æ–­å’Œå¯¼å…¥é€»è¾‘
    # ç§»é™¤åŸæœ‰çš„ if args.openai_model is not None åˆ†æ”¯

    START_DATE = datetime.datetime.now().strftime('%Y-%m-%d')
    START_TIME = datetime.datetime.now().strftime('%H-%M-%S-%f')

    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨args.output_dirä½œä¸ºåŸºç¡€ç›®å½•
    base_output_dir = args.output_dir if hasattr(args, 'output_dir') and args.output_dir else "./results"

    # define SAVE_FOLDER as the timestamp - base model name - mask filling model name
    # create it if it doesn't exist
    # è¡¥å……ï¼šè‹¥argsæ— int8/halfå±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    int8_flag = args.int8 if hasattr(args, 'int8') else False
    half_flag = args.half if hasattr(args, 'half') else False
    precision_string = "int8" if int8_flag else ("fp16" if half_flag else "fp32")

    # è¡¥å……ï¼šè‹¥argsæ— do_top_k/do_top_på±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    do_top_k_flag = args.do_top_k if hasattr(args, 'do_top_k') else False
    do_top_p_flag = args.do_top_p if hasattr(args, 'do_top_p') else False
    sampling_string = "top_k" if do_top_k_flag else ("top_p" if do_top_p_flag else "temp")

    # è¡¥å……ï¼šè‹¥argsæ— output_nameå±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    output_name = args.output_name if hasattr(args, 'output_name') else ""
    output_subfolder = f"{output_name}/" if output_name else ""

    # ç›´æ¥ä½¿ç”¨æœ¬åœ°åŸºç¡€æ¨¡å‹åç§°ï¼Œåˆ é™¤OpenAIç›¸å…³åˆ†æ”¯åˆ¤æ–­
    base_model_name = args.base_model_name.replace('/', '_') if hasattr(args, 'base_model_name') else "gpt2"

    # è¡¥å……ï¼šè‹¥argsæ— scoring_model_nameå±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    scoring_model_name = args.scoring_model_name if hasattr(args, 'scoring_model_name') else ""
    scoring_model_string = (f"-{scoring_model_name}" if scoring_model_name else "").replace('/', '_')

    # è¡¥å……ï¼šè‹¥argsæ— ç›¸å…³å±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    pct_words_masked = args.pct_words_masked if hasattr(args, 'pct_words_masked') else 0.15
    n_perturbation_rounds = args.n_perturbation_rounds if hasattr(args, 'n_perturbation_rounds') else 5
    dataset = args.dataset if hasattr(args, 'dataset') else "WritingPrompts"
    n_samples = args.n_samples if hasattr(args, 'n_samples') else 100

    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨base_output_dirè€Œä¸æ˜¯ç¡¬ç¼–ç çš„tmp_results
    experiment_folder = f"{output_subfolder}{base_model_name}{scoring_model_string}-{args.mask_filling_model_name}-{sampling_string}/{START_DATE}-{START_TIME}-{precision_string}-{pct_words_masked}-{n_perturbation_rounds}-{dataset}-{n_samples}"
    SAVE_FOLDER = os.path.join(base_output_dir, experiment_folder)

    if not os.path.exists(SAVE_FOLDER):
        os.makedirs(SAVE_FOLDER)
    print(f"ğŸ“ ä¿å­˜ç»“æœåˆ°: {os.path.abspath(SAVE_FOLDER)}")
    print(f"ğŸ“ åŸºç¡€è¾“å‡ºç›®å½•: {base_output_dir}")

    # write args to file
    # å…¼å®¹argsä¸ºNamespaceæˆ–å­—å…¸ç±»å‹
    args_dict = vars(args) if hasattr(args, '__dict__') else args
    with open(os.path.join(SAVE_FOLDER, "args.json"), "w") as f:
        json.dump(args_dict, f, indent=4)

    config["START_DATE"] = START_DATE
    config["START_TIME"] = START_TIME
    config["base_model_name"] = base_model_name
    config["SAVE_FOLDER"] = SAVE_FOLDER
    config["API_TOKEN_COUNTER"] = API_TOKEN_COUNTER
    # å…³é”®ä¿®å¤ï¼šç¡®ä¿configä¸­æœ‰output_dir
    config["output_dir"] = base_output_dir


def set_experiment_config(args, config):
    """
    Parses the runtime arguments for setting the experiment configuration.
    """
    # è¡¥å……ï¼šè‹¥argsæ— cache_dirå±æ€§ï¼Œå…œåº•èµ‹å€¼ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    cache_dir = args.cache_dir if hasattr(args, 'cache_dir') else "./cache"
    os.environ["XDG_CACHE_HOME"] = cache_dir
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    print(f"Using cache dir {cache_dir}")

    # è¡¥å……ï¼šæ‰€æœ‰argså±æ€§å‡æ·»åŠ å…œåº•åˆ¤æ–­ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æŠ¥é”™
    mask_filling_model_name = args.mask_filling_model_name if hasattr(args, 'mask_filling_model_name') else "t5-small"
    n_samples = args.n_samples if hasattr(args, 'n_samples') else 100
    batch_size = args.batch_size if hasattr(args, 'batch_size') else 1
    # è¡¥å……ï¼šè‹¥argsæ— n_perturbation_listå±æ€§ï¼Œå…œåº•èµ‹å€¼
    n_perturbation_list = args.n_perturbation_list if hasattr(args, 'n_perturbation_list') else "5"
    n_perturbation_rounds = args.n_perturbation_rounds if hasattr(args, 'n_perturbation_rounds') else 5
    n_similarity_samples = args.n_similarity_samples if hasattr(args, 'n_similarity_samples') else 10

    config["mask_filling_model_name"] = mask_filling_model_name
    config["n_samples"] = n_samples
    config["batch_size"] = batch_size
    config["n_perturbation_list"] = [int(x) for x in n_perturbation_list.split(",")]
    config["n_perturbation_rounds"] = n_perturbation_rounds
    config["n_similarity_samples"] = n_similarity_samples
    config["cache_dir"] = cache_dir