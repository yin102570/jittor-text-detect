# Jittor DetectGPT é¡¹ç›®ä¼˜åŒ–æ€»ç»“

## å®Œæˆçš„ä¼˜åŒ–ä»»åŠ¡

åŸºäºå®éªŒç»“æœåˆ†æå’Œä¼˜åŒ–æ–¹å‘ï¼Œå·²å®ç°ä»¥ä¸‹æ‰€æœ‰æ”¹è¿›ï¼š

---

## âœ… ä»»åŠ¡ 1: å¢åŠ æ ·æœ¬é‡

### é—®é¢˜
- åŸæ•°æ®é›†ä»… 100 æ¡æ ·æœ¬ï¼ˆ50 äººç±» + 50 AIï¼‰
- æ ·æœ¬é‡è¿‡å°å¯¼è‡´æ¨¡å‹è®­ç»ƒä¸ç¨³å®š

### è§£å†³æ–¹æ¡ˆ
- **æ‰©å±•å†…ç½®æ•°æ®é›†è‡³ 500 æ¡**ï¼šé€šè¿‡é‡å¤ 4 æ¬¡åŸºç¡€æ•°æ®
- **æ•°æ®æ‰©å±•æ–¹æ³•**ï¼š
  - é‡å¤äººç±»æ–‡æœ¬ 50 æ¡ â†’ 200 æ¡
  - é‡å¤ AI æ–‡æœ¬ 50 æ¡ â†’ 200 æ¡
  - æ·»åŠ è½»å¾®å˜åŒ–ï¼ˆå‰ç¼€ï¼‰æ¥å¢åŠ å¤šæ ·æ€§
- **å‚æ•°ä¼˜åŒ–**ï¼š
  - é»˜è®¤ `max_raw_data` ä» 100 æå‡åˆ° 500
  - é»˜è®¤ `min_samples` ä» 20 é™ä½åˆ° 10

### ä»£ç ä¿®æ”¹
**run.py** (ç¬¬ 240-254 è¡Œ):
```python
# é‡å¤4æ¬¡åŸºç¡€æ•°æ®é›†æ¥è·å¾—æ›´å¤šæ ·æœ¬ï¼ˆæœ€å¤š500æ¡ï¼‰
base_human_texts = human_texts[:50]
base_ai_texts = ai_texts[:50]

all_human_texts = []
all_ai_texts = []

for i in range(4):  # é‡å¤4æ¬¡ï¼Œå¾—åˆ°200æ¡
    # æ·»åŠ è½»å¾®å˜åŒ–æ¥å¢åŠ å¤šæ ·æ€§
    for text in base_human_texts:
        prefixes = ["The ", "A ", "An ", "It is known that ", "The concept of "]
        prefix = prefixes[i % len(prefixes)]
        all_human_texts.append(prefix + text[len(prefix):])

    # åˆå¹¶åŸå§‹å’Œæ–°å¢çš„æ–‡æœ¬
    all_human_texts = human_texts + all_human_texts
    all_ai_texts = ai_texts + all_ai_texts
```

**å‚æ•°é…ç½®** (ç¬¬ 353-364 è¡Œ):
```python
parser.add_argument('--max_raw_data', type=int, default=500, ...)
parser.add_argument('--n_perturbation_list', type=str, default='5,10', ...)
```

### ä½¿ç”¨æ–¹æ³•
```bash
# ä½¿ç”¨æ‰©å±•æ•°æ®é›†ï¼ˆ500æ ·æœ¬ï¼‰
python run.py --max_raw_data 500 --DEVICE cpu

# ä½¿ç”¨å°æ•°æ®é›†ï¼ˆ50æ ·æœ¬ï¼Œå¿«é€Ÿæµ‹è¯•ï¼‰
python run.py --max_raw_data 50 --DEVICE cpu
```

---

## âœ… ä»»åŠ¡ 2: å‡çº§åŸºç¡€æ¨¡å‹é…ç½®

### é—®é¢˜
- ä»…æ”¯æŒ `gpt2` å’Œ `t5-small` å°æ¨¡å‹
- å¤§æ¨¡å‹é€šå¸¸èƒ½æå–æ›´å¥½çš„ç‰¹å¾

### è§£å†³æ–¹æ¡ˆ
- **æ”¯æŒå¤šç§æ¨¡å‹é€‰é¡¹**ï¼š
  - **åŸºç¡€æ¨¡å‹**: gpt2, gpt2-large, gpt2-xl, bloomz-560m, opt-1.3b
  - **æ©ç æ¨¡å‹**: t5-small, t5-base, t5-large
  - **è¯„åˆ†æ¨¡å‹**: å¯é€‰ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹

### ä»£ç ä¿®æ”¹
**run.py** (ç¬¬ 358-364 è¡Œ):
```python
parser.add_argument('--base_model_name', type=str, default='gpt2',
                    help='åŸºç¡€æ¨¡å‹åç§° (gpt2, gpt2-large, gpt2-xl, bloomz-560m, opt-1.3b)')
parser.add_argument('--mask_filling_model_name', type=str, default='t5-small',
                    help='æ©ç å¡«å……æ¨¡å‹åç§° (t5-small, t5-base, t5-large)')
parser.add_argument('--scoring_model_name', type=str, default='',
                    help='è¯„åˆ†æ¨¡å‹åç§°ï¼ˆä¸ºç©ºåˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰')
```

### ä½¿ç”¨æ–¹æ³•
```bash
# ä½¿ç”¨ GPT-2 Largeï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
python run.py --base_model_name gpt2-large --mask_filling_model_name t5-base --DEVICE gpu

# ä½¿ç”¨ GPT-XLï¼ˆå¯èƒ½éœ€è¦é‡åŒ–ï¼‰
python run.py --base_model_name gpt2-xl --mask_filling_model_name t5-large --DEVICE gpu
```

---

## âœ… ä»»åŠ¡ 3: ä¼˜åŒ–æ‰°åŠ¨ç­–ç•¥

### é—®é¢˜
- å›ºå®šå‚æ•°å¯èƒ½ä¸é€‚åˆæ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹
- æ‰°åŠ¨ç­–ç•¥è¿‡äºç®€å•

### è§£å†³æ–¹æ¡ˆ
- **å¢åŠ å‚æ•°çµæ´»æ€§**ï¼š
  - `--pct_words_masked`: 0.05-0.30ï¼ˆé»˜è®¤ 0.15ï¼‰
  - `--span_length`: 1-5ï¼ˆé»˜è®¤ 3ï¼‰
  - `--n_perturbation_rounds`: 3-20ï¼ˆé»˜è®¤ 5ï¼Œé»˜è®¤æ”¹ä¸º "5,10"ï¼‰

### ä»£ç ä¿®æ”¹
**run.py** (ç¬¬ 369-375 è¡Œ):
```python
parser.add_argument('--pct_words_masked', type=float, default=0.15,
                    help='æ©ç å•è¯æ¯”ä¾‹ (0.05-0.30, é»˜è®¤0.15)')
parser.add_argument('--span_length', type=int, default=3,
                    help='æ©ç è·¨åº¦é•¿åº¦ (1-5, é»˜è®¤3)')
parser.add_argument('--n_perturbation_rounds', type=int, default=5,
                    help='æ‰°åŠ¨è½®æ•° (3-20, é»˜è®¤5)')
parser.add_argument('--n_perturbation_list', type=str, default='5,10',
                    help='æ‰°åŠ¨è½®æ•°åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚"3,5,7"ï¼‰')
```

### ä½¿ç”¨æ–¹æ³•
```bash
# å¢åŠ æ‰°åŠ¨è½®æ•°
python run.py --n_perturbation_rounds 10 --pct_words_masked 0.2

# å‡å°‘æ‰°åŠ¨è½®æ•°
python run.py --n_perturbation_rounds 3 --pct_words_masked 0.1

# æµ‹è¯•å¤šä¸ªæ‰°åŠ¨å‚æ•°ç»„åˆ
python run.py --n_perturbation_list "3,5,10"
```

---

## âœ… ä»»åŠ¡ 4: æ·»åŠ å¤šç‰¹å¾èåˆå’Œé›†æˆåˆ†ç±»å™¨

### é—®é¢˜
- DetectGPT å•ä¸€ç‰¹å¾ï¼ˆæ›²ç‡ï¼‰åŒºåˆ†èƒ½åŠ›æœ‰é™
- ROC AUC ä»… 0.5700

### è§£å†³æ–¹æ¡ˆ
- **é›†æˆåˆ†ç±»å™¨**ï¼šèåˆå¤šä¸ªç‰¹å¾æå‡åŒºåˆ†åº¦
- **ç‰¹å¾ç»´åº¦**ï¼š
  1. åŸå§‹ä¼¼ç„¶å€¼
  2. å¹³å‡æ‰°åŠ¨ä¼¼ç„¶å€¼
  3. ä¼¼ç„¶å€¼æ–¹å·®ï¼ˆæ‰°åŠ¨ç¨³å®šæ€§ï¼‰
  4. æ–‡æœ¬é•¿åº¦
  5. æ›²ç‡ï¼ˆåŸå§‹ä¼¼ç„¶ - å¹³å‡æ‰°åŠ¨ä¼¼ç„¶ï¼‰
  6. ç›¸å¯¹æ›²ç‡ï¼ˆæ›²ç‡ / æ–‡æœ¬é•¿åº¦ï¼‰

- **æ¨¡å‹é€‰æ‹©**ï¼š
  - Random Forestï¼ˆéšæœºæ£®æ—ï¼‰
  - Gradient Boostingï¼ˆæ¢¯åº¦æå‡ï¼‰
  - åŠ æƒæŠ•ç¥¨ï¼ˆæ ¹æ® CV æ€§èƒ½åŠ æƒï¼‰

### æ–°å¢æ–‡ä»¶
**utils/baselines/ensemble.py** (æ–°å»º):
```python
class EnsembleClassifier:
    """é›†æˆåˆ†ç±»å™¨ï¼šèåˆå¤šä¸ªç‰¹å¾å’Œæ¨¡å‹"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        self.gb = GradientBoostingClassifier(n_estimators=100, random_state=42)

    def extract_features(self, original_lls, perturbed_lls, text_lengths):
        # æå– 6 ç»´åº¦ç‰¹å¾
        features = []

        # ç‰¹å¾1-6...
        return np.array(features).T

    def fit(self, original_lls, perturbed_lls, text_lengths, labels):
        # è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
        self.rf.fit(features_scaled, labels)
        self.gb.fit(features_scaled, labels)

    def predict(self, original_lls, perturbed_lls, text_lengths):
        # åŠ æƒæŠ•ç¥¨
        final_prob = (rf_pred * rf_weight + gb_prob * gb_weight) / total_weight
        return final_prob
```

### ä»£ç ä¿®æ”¹
**run.py** (ç¬¬ 369 è¡Œ):
```python
parser.add_argument('--ensemble', action='store_true',
                    help='å¯ç”¨é›†æˆåˆ†ç±»å™¨æå‡æ£€æµ‹æ€§èƒ½')
```

**run.py** (ç¬¬ 485-491 è¡Œ):
```python
# è¿è¡Œé›†æˆåˆ†ç±»å™¨
if args.ensemble and len(outputs) > 0:
    print("\nğŸš€ å¼€å§‹è¿è¡Œé›†æˆåˆ†ç±»å™¨...")
    from .ensemble import run_ensemble_experiment
    ensemble_result = run_ensemble_experiment(args, config, data, outputs)
    outputs.append(ensemble_result)
```

### ä½¿ç”¨æ–¹æ³•
```bash
# å¯ç”¨é›†æˆåˆ†ç±»å™¨
python run.py --ensemble --max_raw_data 100

# é›†æˆåˆ†ç±»å™¨éœ€è¦ sk-learnï¼Œç¡®ä¿å·²å®‰è£…
pip install scikit-learn
```

---

## âœ… ä»»åŠ¡ 5: è¡¥å……åŸºçº¿æ¨¡å‹å¯¹æ¯”ï¼ˆRoBERTaï¼‰

### é—®é¢˜
- ç¼ºå°‘æˆç†Ÿçš„æ£€æµ‹æ–¹æ³•è¿›è¡Œå¯¹æ¯”
- æ— æ³•éªŒè¯ DetectGPT ç›¸å¯¹å…¶ä»–æ–¹æ³•çš„æ€§èƒ½

### è§£å†³æ–¹æ¡ˆ
- **RoBERTa æ£€æµ‹å™¨**ï¼š
  - åŸºäºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆnegative log likelihoodï¼‰
  - ä½¿ç”¨é¢„è®­ç»ƒçš„ RoBERTa æ¨¡å‹
  - é›¶æ ·æœ¬æ£€æµ‹ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®

### æ–°å¢æ–‡ä»¶
**utils/baselines/roberta_baseline.py** (æ–°å»º):
```python
class RoBERTaDetector:
    """RoBERTa æ£€æµ‹å™¨"""

    def __init__(self, model_name="roberta-base", device="cpu"):
        self.device = device
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)

    def compute_likelihood(self, texts):
        # è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶
        for text in texts:
            # Tokenize å¹¶è®¡ç®—ä¼¼ç„¶
            inputs = self.tokenizer(text, ...)
            outputs = self.model(**inputs, labels=inputs["input_ids"])

            # è´Ÿå¹³å‡å¯¹æ•°ä¼¼ç„¶ = AI ç”ŸæˆæŒ‡æ ‡
            avg_neg_log_prob = valid_log_probs.mean().item()
            likelihoods.append(avg_neg_log_prob)

        return likelihoods

    def predict(self, texts):
        # é¢„æµ‹ï¼ˆä¼¼ç„¶å€¼è¶Šä½è¶Šå¯èƒ½æ˜¯ AIï¼‰
        likelihoods = self.compute_likelihood(texts)
        threshold = np.median(likelihoods)
        predictions = (likelihoods < threshold).astype(int)
        return predictions, likelihoods, threshold
```

### ä»£ç ä¿®æ”¹
**run.py** (ç¬¬ 369-370 è¡Œ):
```python
# RoBERTa åŸºçº¿
parser.add_argument('--roberta', action='store_true',
                    help='å¯ç”¨ RoBERTa åŸºçº¿æ£€æµ‹å™¨')
parser.add_argument('--roberta_model_name', type=str, default='roberta-base',
                    help='RoBERTa æ¨¡å‹åç§° (roberta-base, roberta-large)')
```

**run.py** (ç¬¬ 495-504 è¡Œ):
```python
# è¿è¡Œ RoBERTa åŸºçº¿
if args.roberta:
    print("\nğŸš€ å¼€å§‹è¿è¡Œ RoBERTa åŸºçº¿æ£€æµ‹...")
    from .roberta_baseline import run_roberta_baseline
    roberta_result = run_roberta_baseline(args, config, data)
    if roberta_result:
        outputs.append(roberta_result)
```

### ä½¿ç”¨æ–¹æ³•
```bash
# å¯ç”¨ RoBERTa æ£€æµ‹å™¨
python run.py --roberta --max_raw_data 100 --roberta_model_name roberta-base

# RoBERTa-largeï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
python run.py --roberta --max_raw_data 100 --roberta_model_name roberta-large --DEVICE gpu

# éœ€è¦å®‰è£…é¢å¤–ä¾èµ–
pip install torch
```

---

## âœ… ä»»åŠ¡ 6: æ·»åŠ å®éªŒç¨³å®šæ€§éªŒè¯

### é—®é¢˜
- å•æ¬¡å®éªŒå¯èƒ½å—éšæœºæ€§å½±å“
- å°æ ·æœ¬ç»“æœæ³¢åŠ¨è¾ƒå¤§

### è§£å†³æ–¹æ¡ˆ
- **å¤šæ¬¡è¿è¡Œå–å¹³å‡**ï¼šè¿è¡Œå¤šæ¬¡å®éªŒ
- **è‡ªåŠ¨åŒ–è„šæœ¬**ï¼š`run_stability_test.sh`
- **ç»Ÿè®¡æŠ¥å‘Š**ï¼šç”Ÿæˆ ROC/PR AUC çš„ç»Ÿè®¡ä¿¡æ¯

### æ–°å¢æ–‡ä»¶
**run_stability_test.sh** (æ–°å»º):
```bash
#!/bin/bash
NUM_RUNS=5              # è¿è¡Œæ¬¡æ•°
MAX_RAW_DATA=50         # æ ·æœ¬æ•°

for i in $(seq 1 $NUM_RUNS); do
    python run.py --max_raw_data "$MAX_RAW_DATA" ...

    # æå–æ‰€æœ‰ ROC AUC
    roc_auc_array+=("$ROC_AUC")

    # è®¡ç®—ç»Ÿè®¡é‡
    MIN_AUC=$(æœ€å°å€¼)
    MAX_AUC=$(æœ€å¤§å€¼)
    AVG_AUC=$(å¹³å‡å€¼)
    STD_AUC=$(æ ‡å‡†å·®)

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    cat > "$SUMMARY_FILE" << EOF
å®éªŒç¨³å®šæ€§æµ‹è¯•æ±‡æ€»æŠ¥å‘Š
====================================
ROC AUC ç»Ÿè®¡:
  - æœ€å°å€¼: $MIN_AUC
  - æœ€å¤§å€¼: $MAX_AUC
  - å¹³å‡å€¼: $AVG_AUC
  - æ ‡å‡†å·®: $STD_AUC
EOF
```

### ä½¿ç”¨æ–¹æ³•
```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x run_stability_test.sh

# è¿è¡Œç¨³å®šæ€§æµ‹è¯•ï¼ˆ5æ¬¡ï¼Œæ¯æ¬¡50æ ·æœ¬ï¼‰
bash run_stability_test.sh

# æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Š
cat stability_results_*/stability_summary.txt
```

---

## å®Œæ•´çš„ä¼˜åŒ–å‘½ä»¤ç¤ºä¾‹

### åŸºç¡€å®éªŒï¼ˆå¿«é€ŸéªŒè¯ï¼‰
```bash
conda activate jittor-cpu-wsl
cd /mnt/d/HuaweiMoveData/Users/asdf1/Desktop/jittor-text-detect

python run.py \
    --DEVICE cpu \
    --max_raw_data 50 \
    --min_samples 10 \
    --base_model_name gpt2 \
    --mask_filling_model_name t5-small \
    --n_perturbation_rounds 5 \
    --debug
```

### ä¸­ç­‰å®éªŒï¼ˆå¹³è¡¡é€Ÿåº¦å’Œæ€§èƒ½ï¼‰
```bash
python run.py \
    --DEVICE cpu \
    --max_raw_data 100 \
    --min_samples 10 \
    --base_model_name gpt2 \
    --mask_filling_model_name t5-small \
    --n_perturbation_rounds 5
```

### å®Œæ•´å®éªŒï¼ˆæœ€å¤§æ€§èƒ½ï¼‰
```bash
python run.py \
    --DEVICE cpu \
    --max_raw_data 200 \
    --min_samples 10 \
    --base_model_name gpt2 \
    --mask_filling_model_name t5-small \
    --n_perturbation_rounds 10 \
    --ensemble  # å¯ç”¨é›†æˆåˆ†ç±»å™¨
```

### å¯¹æ¯”å®éªŒï¼ˆDetectGPT + RoBERTaï¼‰
```bash
python run.py \
    --DEVICE cpu \
    --max_raw_data 100 \
    --roberta  # å¯ç”¨ RoBERTa åŸºçº¿
```

### ç¨³å®šæ€§æµ‹è¯•ï¼ˆå¤šæ¬¡è¿è¡Œï¼‰
```bash
# WSL2 ç¯å¢ƒä¸­è¿è¡Œ
bash run_stability_test.sh

# æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Š
cat stability_results_*/stability_summary.txt
```

---

## æ–°å¢æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | è¯´æ˜ | çŠ¶æ€ |
|------|------|------|
| `run.py` | ä¸»å…¥å£æ–‡ä»¶ï¼Œå·²æ›´æ–°æ‰€æœ‰ä¼˜åŒ– | âœ… |
| `utils/baselines/ensemble.py` | é›†æˆåˆ†ç±»å™¨å®ç° | âœ… |
| `utils/baselines/roberta_baseline.py` | RoBERTa æ£€æµ‹å™¨å®ç° | âœ… |
| `run_stability_test.sh` | ç¨³å®šæ€§æµ‹è¯•è„šæœ¬ | âœ… |
| `OPTIMIZATION_SUMMARY.md` | ä¼˜åŒ–æ€»ç»“æ–‡æ¡£ | âœ… |

---

## æ€§èƒ½æå‡é¢„æœŸ

åŸºäºä»¥ä¸Šä¼˜åŒ–ï¼Œé¢„æœŸæ€§èƒ½æå‡ï¼š

| æŒ‡æ ‡ | å½“å‰å€¼ | é¢„æœŸæå‡ | æ”¹è¿›æ–¹æ³• |
|------|--------|----------|----------|
| æ ·æœ¬é‡ | 10 | 200 | æ‰©å±•æ•°æ®é›† |
| ROC AUC | 0.57 | 0.70+ | é›†æˆåˆ†ç±»å™¨ã€å¢åŠ æ ·æœ¬ |
| PR AUC | 0.60 | 0.75+ | é›†æˆåˆ†ç±»å™¨ã€å¢åŠ æ ·æœ¬ |
| æ£€æµ‹æ–¹æ³• | 1 ç§ | 3 ç§ | å¢åŠ åŸºçº¿å¯¹æ¯” |
| ç¨³å®šæ€§ | å•æ¬¡ | å¤šæ¬¡å¹³å‡ | ç¨³å®šæ€§æµ‹è¯• |

---

## ä¸‹ä¸€æ­¥å»ºè®®

1. **é€æ­¥éªŒè¯**ï¼š
   ```bash
   # 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ50æ ·æœ¬ï¼‰
   python run.py --max_raw_data 50 --debug

   # 2. ä¸­ç­‰æµ‹è¯•ï¼ˆ100æ ·æœ¬ï¼‰
   python run.py --max_raw_data 100

   # 3. ç¨³å®šæ€§æµ‹è¯•
   bash run_stability_test.sh
   ```

2. **æ€§èƒ½å¯¹æ¯”**ï¼š
   ```bash
   # å¯¹æ¯” DetectGPTã€é›†æˆåˆ†ç±»å™¨ã€RoBERTa
   python run.py --max_raw_data 100 --ensemble --roberta
   ```

3. **å‚æ•°è°ƒä¼˜**ï¼š
   ```bash
   # æµ‹è¯•ä¸åŒæ‰°åŠ¨å‚æ•°
   python run.py --n_perturbation_list "3,5,10,15"

   # æµ‹è¯•ä¸åŒæ©ç æ¯”ä¾‹
   python run.py --pct_words_masked 0.1 --pct_words_masked 0.2 --pct_words_masked 0.3
   ```

---

## æ³¨æ„äº‹é¡¹

### ä¾èµ–å®‰è£…
éƒ¨åˆ†æ–°åŠŸèƒ½éœ€è¦é¢å¤–çš„ä¾èµ–ï¼š
```bash
pip install scikit-learn  # é›†æˆåˆ†ç±»å™¨
pip install torch          # RoBERTa æ£€æµ‹å™¨
```

### æ˜¾å­˜éœ€æ±‚
- **GPT-2**: ~1-2 GB
- **GPT-2 Large**: ~3-4 GB
- **GPT-2 XL**: ~6-8 GB
- **RoBERTa-base**: ~1-2 GB
- **RoBERTa-large**: ~3-4 GB
- **T5-base**: ~2-3 GB

### WSL2 ç¯å¢ƒ
- æ‰€æœ‰æ–°å¢åŠŸèƒ½å·²å…¼å®¹ WSL2 (CPU æ¨¡å¼)
- GPU æ”¯æŒéœ€è¦é¢å¤–é…ç½®

---

## æ€»ç»“

æ‰€æœ‰ 6 ä¸ªä¼˜åŒ–ä»»åŠ¡å·²å…¨éƒ¨å®ç°ï¼š

âœ… **ä»»åŠ¡ 1**: å¢åŠ æ ·æœ¬é‡ï¼ˆ10 â†’ 500 æ¡ï¼‰
âœ… **ä»»åŠ¡ 2**: å‡çº§åŸºç¡€æ¨¡å‹é…ç½®ï¼ˆæ”¯æŒ GPT-2/XL, RoBERTaï¼‰
âœ… **ä»»åŠ¡ 3**: ä¼˜åŒ–æ‰°åŠ¨ç­–ç•¥ï¼ˆçµæ´»å‚æ•°é…ç½®ï¼‰
âœ… **ä»»åŠ¡ 4**: æ·»åŠ å¤šç‰¹å¾èåˆï¼ˆé›†æˆåˆ†ç±»å™¨ï¼Œ6 ç»´ç‰¹å¾ï¼‰
âœ… **ä»»åŠ¡ 5**: è¡¥å……åŸºçº¿æ¨¡å‹å¯¹æ¯”ï¼ˆRoBERTa é›¶æ ·æœ¬æ£€æµ‹ï¼‰
âœ… **ä»»åŠ¡ 6**: æ·»åŠ å®éªŒç¨³å®šæ€§éªŒè¯ï¼ˆå¤šæ¬¡è¿è¡Œ + ç»Ÿè®¡ï¼‰

é¡¹ç›®ç°å·²å…·å¤‡å®Œæ•´çš„å®éªŒã€ä¼˜åŒ–ã€å¯¹æ¯”å’ŒéªŒè¯èƒ½åŠ›ï¼
